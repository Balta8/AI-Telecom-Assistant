# src/nodes/faq_node.py

from langchain.tools import BaseTool
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.memory import ConversationBufferMemory
from utils.retrievers import RetrieverManager
from config import require_openai_key, LLM_MODEL
from constants import RECENT_MESSAGES_LIMIT, MAX_DOCS_FOR_FAQ
from typing import Optional, Type
from pydantic import BaseModel, Field

class FaqInput(BaseModel):
    """Input for FAQ tool."""
    question: str = Field(description="Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø­ÙˆÙ„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©")

class FaqTool(BaseTool):
    """Tool for answering frequently asked questions."""
    name: str = "faq_tool"
    description: str = "Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙƒØ±Ø±Ø© Ø­ÙˆÙ„ Ø®Ø¯Ù…Ø§Øª Ø´Ø±ÙƒÙ‡ Ù…ØªØ®ØµØµÙ‡ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„Ø§ØªØŒ Ø§Ù„Ø´Ø­Ù†ØŒ Ø§Ù„Ø¥Ù„ØºØ§Ø¡ØŒ ÙˆØ§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©"
    args_schema: Type[BaseModel] = FaqInput

    def __init__(self, retriever_manager: RetrieverManager, memory: ConversationBufferMemory, model_name: str = LLM_MODEL):
        super().__init__()
        # Store components as private attributes to avoid Pydantic issues
        self._retriever = retriever_manager.retrievers.get("faq")
        if not self._retriever:
            raise ValueError("FAQ retriever Ù…Ø´ Ù…ØªØ¹Ø±Ù")

        self._llm = ChatOpenAI(api_key=require_openai_key(), model_name=model_name, temperature=0)
        self._memory = memory

        self._prompt = ChatPromptTemplate.from_template("""
# Ø¯ÙˆØ±Ùƒ
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø®Ø¯Ù…Ø© Ø¹Ù…Ù„Ø§Ø¡ ÙÙŠ Ø´Ø±ÙƒØ© Ø§ØªØµØ§Ù„Ø§Øª.

# Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©

## ðŸ• ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:
{history}

## â“ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯:
{question}

## ðŸ“š Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© (FAQs):
{context}

# Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø±Ø¯

1. **Ø§Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©** Ø¹Ù† Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
2. **Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:**
   - Ø§Ø°ÙƒØ±Ù‡Ø§ Ø¨ÙˆØ¶ÙˆØ­ ÙˆØ¥ÙŠØ¬Ø§Ø²
   - Ø£Ø¶Ù Ø®Ø·ÙˆØ§Øª Ø¹Ù…Ù„ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ©
   - Ø£Ø¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø© Ø¥Ø¶Ø§ÙÙŠØ© Ù…ÙÙŠØ¯Ø© Ø¥Ù† ÙˆÙØ¬Ø¯Øª
3. **Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø©:**
   - Ù‚Ù„ Ø¨ÙˆØ¶ÙˆØ­: "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ Ø£Ù…Ù„Ùƒ Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„"
   - Ø§Ù‚ØªØ±Ø­ Ø¨Ø¯ÙŠÙ„: "Ù„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙŠ..."
   - Ø£Ùˆ: "ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ø¯Ø¹Ù… Ø¹Ù„Ù‰: Ù¡Ù¦Ù "

# Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø±Ø¯
- ÙˆØ¯ÙˆØ¯ ÙˆÙ…Ø¨Ø§Ø´Ø±
- Ù„Ø§ ØªØ²ÙŠØ¯ Ø¹Ù† 100 ÙƒÙ„Ù…Ø©
- Ø§Ø³ØªØ®Ø¯Ù… numbering Ù„Ù„Ø®Ø·ÙˆØ§Øª
- Ø£Ø¶Ù emoji Ø®ÙÙŠÙ (Ù…Ø«Ù„ âœ… Ø£Ùˆ ðŸ“ž)

# Ø£Ù…Ø«Ù„Ø©

**Ù…Ø«Ø§Ù„ 1 - Ø³Ø¤Ø§Ù„ ÙˆØ§Ø¶Ø­:**
â“ ÙƒÙŠÙ Ø£Ø´Ø­Ù† Ø±ØµÙŠØ¯ØŸ
âœ… ÙŠÙ…ÙƒÙ†Ùƒ Ø´Ø­Ù† Ø§Ù„Ø±ØµÙŠØ¯ Ø¨Ù€:
1. ÙƒØ±ÙˆØª Ø§Ù„Ø´Ø­Ù† Ù…Ù† Ø£ÙŠ Ù…Ù†ÙØ° Ø¨ÙŠØ¹
2. Ù…Ù† Ø®Ù„Ø§Ù„ ÙÙˆØ¯Ø§ÙÙˆÙ† ÙƒØ§Ø´
3. Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚

**Ù…Ø«Ø§Ù„ 2 - Ø³Ø¤Ø§Ù„ ØºØ§Ù…Ø¶:**
â“ Ø¹Ù†Ø¯ÙŠ Ù…Ø´ÙƒÙ„Ø©
âœ… Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©ØŸ
- Ù‡Ù„ Ù‡ÙŠ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ©ØŸ
- Ø£Ù… ÙÙŠ Ø§Ù„ÙØ§ØªÙˆØ±Ø©ØŸ
- Ø£Ù… Ø´ÙŠØ¡ Ø¢Ø®Ø±ØŸ

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
""")

    def _run(self, question: str, session_id: Optional[str] = None) -> str:
        """Run the FAQ tool."""
        docs = self._retriever.get_relevant_documents(question)
        if not docs:
            response = "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ø¯Ø¹Ù… Ø¹Ù„Ù‰: Ù¡Ù¦Ù "
        else:
            context = "\n".join([d.page_content for d in docs[:MAX_DOCS_FOR_FAQ]])
            
            # Get chat history from the agent's memory
            history_text = ""
            if self._memory and hasattr(self._memory, 'chat_memory'):
                messages = self._memory.chat_memory.messages
                # Get last few messages for context
                recent_messages = messages[-RECENT_MESSAGES_LIMIT:] if len(messages) > RECENT_MESSAGES_LIMIT else messages
                history_text = "\n".join([
                    f"{'User' if hasattr(msg, 'content') and 'Human' in str(type(msg)) else 'Assistant'}: {msg.content}" 
                    for msg in recent_messages if hasattr(msg, 'content')
                ])
            
            chain_input = {
                "question": question, 
                "context": context,
                "history": history_text
            }
            response = self._llm.predict(self._prompt.format(**chain_input)).strip()

        return response

    async def _arun(self, question: str, session_id: Optional[str] = None) -> str:
        """Async run method."""
        return self._run(question, session_id)

