# src/nodes/package_recommendation_node.py

from langchain.tools import BaseTool
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory
from utils.retrievers import RetrieverManager
from config import require_openai_key, LLM_MODEL
from constants import (
    LISTING_KEYWORDS, MAX_DOCS_FOR_RECOMMENDATION, MAX_DOCS_FOR_LISTING,
    DIVERSE_PACKAGE_QUERIES, MAX_DOCS_PER_CATEGORY, RECENT_MESSAGES_LIMIT
)
from typing import Optional, Type
from pydantic import BaseModel, Field

class PackageRecommendationInput(BaseModel):
    """Input for package recommendation tool."""
    user_needs: str = Field(description="Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØªÙØ¶ÙŠÙ„Ø§ØªÙ‡ Ù„ØªØ±Ø´ÙŠØ­ Ø§Ù„Ø¨Ø§Ù‚Ø©")

class PackageRecommendationTool(BaseTool):
    """Tool for recommending the best package based on user needs."""
    name: str = "package_recommendation_tool"
    description: str = """
Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø© Ù„ØªØ±Ø´ÙŠØ­ Ø¨Ø§Ù‚Ø© Ø£Ùˆ Ø¹Ø±Ø¶ ÙƒÙ„ Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.

Ù…ØªÙ‰ ØªØ³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø©:
âœ… "Ø¹Ø§ÙŠØ² Ø¨Ø§Ù‚Ø© Ù„Ù„Ù…ÙƒØ§Ù„Ù…Ø§Øª Ø¨Ø­Ø¯ Ù¡Ù Ù Ø¬"
âœ… "Ø±Ø´Ø­Ù„ÙŠ Ø¨Ø§Ù‚Ø© Ù…Ù†Ø§Ø³Ø¨Ø©"
âœ… "Ø§ÙŠÙ‡ ÙƒÙ„ Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©ØŸ"
âœ… "Ø¹Ø§ÙŠØ² Ø§Ø¹Ø±Ù Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©"

Ù…ØªÙ‰ Ù„Ø§ ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§:
âŒ "ØªÙØ§ØµÙŠÙ„ ÙÙ„ÙŠÙƒØ³ Ù§Ù " â†’ Ø§Ø³ØªØ®Ø¯Ù… package_info_tool
âŒ "Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Plus 155" â†’ Ø§Ø³ØªØ®Ø¯Ù… package_info_tool

Ø§Ù„Ù…Ø¯Ø®Ù„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø£Ùˆ Ø·Ù„Ø¨ Ø¹Ø±Ø¶ ÙƒÙ„ Ø§Ù„Ø¨Ø§Ù‚Ø§Øª
"""
    args_schema: Type[BaseModel] = PackageRecommendationInput

    def __init__(self, retriever_manager: RetrieverManager, memory: ConversationBufferMemory):
        super().__init__()
        self._retriever_manager = retriever_manager
        self._memory = memory
        self._llm = ChatOpenAI(model=LLM_MODEL, temperature=0.3, api_key=require_openai_key())

        self._prompt = PromptTemplate(
            input_variables=["query", "docs", "preferences", "history"],
            template="""
# Ø¯ÙˆØ±Ùƒ
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙÙŠ Ø´Ø±ÙƒØ© Ø§ØªØµØ§Ù„Ø§Øª. Ù…Ù‡Ù…ØªÙƒ: ØªØ±Ø´ÙŠØ­ Ø§Ù„Ø¨Ø§Ù‚Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ø£Ùˆ Ø¹Ø±Ø¶ Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.

# Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©

## ðŸ“ Ø·Ù„Ø¨ Ø§Ù„Ø¹Ù…ÙŠÙ„:
{query}

## ðŸ“¦ Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© (Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ù‡Ø°Ù‡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©):
{docs}

## ðŸ• Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:
{history}

## ðŸŽ¯ ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ø¹Ù…ÙŠÙ„ (Ø¥Ù† ÙˆÙØ¬Ø¯Øª):
{preferences}

# Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¹Ù…Ù„

## Ø¥Ø°Ø§ Ø·Ù„Ø¨ "ÙƒÙ„ Ø§Ù„Ø¨Ø§Ù‚Ø§Øª" Ø£Ùˆ "Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©":
Ø§Ø¹Ø±Ø¶ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø¸Ù…:

ðŸ“¦ **Ø¨Ø§Ù‚Ø§Øª ÙÙ„ÙŠÙƒØ³:**
â€¢ ÙÙ„ÙŠÙƒØ³ Ù§Ù  â€” [Ø§Ù„ØªÙØ§ØµÙŠÙ„] â€” Ø§Ù„Ø³Ø¹Ø±: [Ø§Ù„Ø³Ø¹Ø±]
â€¢ ÙÙ„ÙŠÙƒØ³ Ù¡Ù Ù  â€” [Ø§Ù„ØªÙØ§ØµÙŠÙ„] â€” Ø§Ù„Ø³Ø¹Ø±: [Ø§Ù„Ø³Ø¹Ø±]

ðŸ“¦ **Ø¨Ø§Ù‚Ø§Øª Plus:**
â€¢ Plus 155 â€” [Ø§Ù„ØªÙØ§ØµÙŠÙ„] â€” Ø§Ù„Ø³Ø¹Ø±: [Ø§Ù„Ø³Ø¹Ø±]

ðŸ’¡ Ù‡Ù„ ØªØ±ÙŠØ¯ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØ«Ø± Ø¹Ù† Ø¨Ø§Ù‚Ø© Ù…Ø¹ÙŠÙ†Ø©ØŸ

## Ø¥Ø°Ø§ Ø·Ù„Ø¨ ØªØ±Ø´ÙŠØ­ Ø¨Ø§Ù‚Ø©:

### Ø®Ø·ÙˆØ© 1: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª
- Ø§Ù„Ù…ÙŠØ²Ø§Ù†ÙŠØ©: [Ù‡Ù„ Ø°ÙƒØ±Ù‡Ø§ØŸ]
- Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: [Ø¥Ù†ØªØ±Ù†Øª / Ù…ÙƒØ§Ù„Ù…Ø§Øª / Ù…Ø²ÙŠØ¬]
- Ø£ÙŠ ØªÙØ¶ÙŠÙ„Ø§Øª Ø£Ø®Ø±Ù‰ Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚

### Ø®Ø·ÙˆØ© 2: Ø§Ù„ØªØ±Ø´ÙŠØ­
Ø§Ø®ØªØ± Ø§Ù„Ø¨Ø§Ù‚Ø© Ø§Ù„Ø£Ù†Ø³Ø¨ Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø£Ø¹Ù„Ø§Ù‡

### Ø®Ø·ÙˆØ© 3: Ø§Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØµÙŠØ©

âœ… **Ø§Ù„Ø¨Ø§Ù‚Ø© Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡Ø§:**
ðŸ“¦ [Ø§Ø³Ù… Ø§Ù„Ø¨Ø§Ù‚Ø©]

**Ù„Ù…Ø§Ø°Ø§ØŸ**
â€¢ [Ø³Ø¨Ø¨ 1]
â€¢ [Ø³Ø¨Ø¨ 2]

**Ø§Ù„Ø³Ø¹Ø±:** [Ø§Ù„Ø³Ø¹Ø±]
**Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:** [Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª]

ðŸ’¡ Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ Ø¨Ø§Ù‚Ø© Ø£Ø®Ø±Ù‰ØŸ

# Ù‚ÙˆØ§Ø¹Ø¯ ØµØ§Ø±Ù…Ø© ðŸš«
1. Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ù‚Ø³Ù… "Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©" Ø£Ø¹Ù„Ø§Ù‡
2. Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø£Ø³Ø¹Ø§Ø± Ø£Ùˆ Ù…Ù…ÙŠØ²Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©
3. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†Ø§Ù‚ØµØ©ØŒ Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„ ÙˆØ§Ø­Ø¯ Ù…Ø­Ø¯Ø¯ ÙÙ‚Ø·
4. Ù„Ø§ ØªØ°ÙƒØ± Ø£ÙƒØ«Ø± Ù…Ù† 3 Ø¨Ø§Ù‚Ø§Øª ÙÙŠ Ø§Ù„ØªØ±Ø´ÙŠØ­
5. Ø±ØªØ¨ Ø§Ù„Ø¨Ø§Ù‚Ø§Øª Ù…Ù† Ø§Ù„Ø£Ù†Ø³Ø¨ Ù„Ù„Ø£Ù‚Ù„ Ù…Ù†Ø§Ø³Ø¨Ø©

# Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø±Ø¯
- ÙˆØ§Ø¶Ø­ ÙˆÙ…Ø¨Ø§Ø´Ø±
- Ù…Ù†Ø¸Ù… (bullets Ø£Ùˆ numbering)
- ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ emoji Ø¨Ø³ÙŠØ·
- Ù„Ø§ ÙŠØ²ÙŠØ¯ Ø¹Ù† 200 ÙƒÙ„Ù…Ø© Ø¥Ù„Ø§ Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨Øª ÙƒÙ„ Ø§Ù„Ø¨Ø§Ù‚Ø§Øª

Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
"""
        )
        self._chain = LLMChain(llm=self._llm, prompt=self._prompt)

    def _run(self, user_needs: str, session_id: Optional[str] = None) -> str:
        """Run the package recommendation tool."""
        # Check if user is asking for all packages
        query_lower = user_needs.lower()
        is_listing_request = any(word in query_lower for word in LISTING_KEYWORDS)
        
        if is_listing_request:
            # Get diverse packages for listing
            all_docs = []
            
            # Strategy: Get diverse packages by querying different terms
            for query in DIVERSE_PACKAGE_QUERIES:
                docs = self._retriever_manager.get_documents(query, "package")
                all_docs.extend(docs[:MAX_DOCS_PER_CATEGORY])
            
            # Remove duplicates based on content
            seen = set()
            unique_docs = []
            for doc in all_docs:
                content = doc.page_content
                if content not in seen:
                    seen.add(content)
                    unique_docs.append(doc)
            
            # Format with better structure (numbered list)
            docs_text = "\n".join([f"{i+1}. {doc.page_content}" 
                                  for i, doc in enumerate(unique_docs[:MAX_DOCS_FOR_LISTING])])
        else:
            # Get specific recommendations
            docs = self._retriever_manager.get_documents(user_needs, "package")
            if not docs:
                return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£Ø¬Ø¯ Ø¨Ø§Ù‚Ø§Øª Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ø§Ø­ØªÙŠØ§Ø¬Ø§ØªÙƒ. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆØ¶ÙŠØ­ Ù…ØªØ·Ù„Ø¨Ø§ØªÙƒ Ø£ÙƒØ«Ø±ØŸ"
            
            docs_text = "\n".join([f"- {doc.page_content}" 
                                  for doc in docs[:MAX_DOCS_FOR_RECOMMENDATION]])

        # Get chat history from the agent's memory
        history_text = ""
        if self._memory and hasattr(self._memory, 'chat_memory'):
            messages = self._memory.chat_memory.messages
            recent_messages = messages[-RECENT_MESSAGES_LIMIT:] if len(messages) > RECENT_MESSAGES_LIMIT else messages
            history_text = "\n".join([
                f"{'User' if hasattr(msg, 'content') and 'Human' in str(type(msg)) else 'Assistant'}: {msg.content}" 
                for msg in recent_messages if hasattr(msg, 'content')
            ])

        response = self._chain.run(
            query=user_needs,
            docs=docs_text,
            preferences="",  # We'll get preferences from conversation context
            history=history_text
        )

        return response

    async def _arun(self, user_needs: str, session_id: Optional[str] = None) -> str:
        """Async run method."""
        return self._run(user_needs, session_id)

